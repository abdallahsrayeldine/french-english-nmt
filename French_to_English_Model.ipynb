{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdallahsrayeldine/Seq2Seq-Translator-Model-with-Attention/blob/main/French_to_English_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho2RzxjO-S2w"
      },
      "source": [
        "# ** SEQ2SEQ Translation Model with CAUTION MERCHISMS **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bx7zuIf-hna"
      },
      "source": [
        "# Installing Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKmx-l6mxgR8"
      },
      "source": [
        "In this section, we install the `Einops` and` Tensorflow-Text 'Packages. These packages are essential for data manipulation and natural language processing with tensorflow.\n",
        "\n",
        "- `einops` is a python library that allows flexible and expressive manipulation of tensor axes. This Facilitates Dimensal Reordering and Data Processing in Neural Networks.\n",
        "- `Tensorflow-Textt` is a tensorflow extension specificly designed for natural language processing (NLP). It provides various text preprocessing features and text encoding methods for use with nlp models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDTen0627opF"
      },
      "outputs": [],
      "source": [
        "!pip install einops\n",
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olay_ypX-lZt"
      },
      "source": [
        "# Libraries and modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-wgZ5zyxif8"
      },
      "source": [
        "In this section, we import the libraries and modules Needed for the rest of the code. Here's an Explanation of the Main Imports:\n",
        "\n",
        "- `Numpy` is a python library used to perform numbers and operations on multidimensal arumbs (real nurse, vectors, matrices, etc.).\n",
        "- `Typing` is a python that module provids functionality for annotating types in code. It is used here to specific the types of function arguments and return values.\n",
        "- `Einops` has already been explanéed previously during its installation.\n",
        "- `MATPLOTLIB.PYPLOT 'is used to create visualizations, included graphs and studs.\n",
        "- `MATPLOTLIB.TICKER 'IS USED to Manage Ticks and Labels on the Axes of Graphs.\n",
        "\n",
        "Finlly, we import `Tensorflow` and` Tensorflow_Text`, which are the Main Libraries for Building Neural Network Models and Natural Language Processing With Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBU4XMi77xHc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTnnVle4_CBb"
      },
      "source": [
        "# Data handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNUNLPxP_OBg"
      },
      "source": [
        "## Verification class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcCJZWmZxkSh"
      },
      "source": [
        "In this sub-part, we define a class called `Shapechecker 'which helps us to check the forms of tensors when manipulating data. This class is particularly useful for ensuring the compatibility of dimensions when using neural networks models.\n",
        "\n",
        "The Shapechecker 'class has a `` __call_ `` method, which takes a tensor and a list of axes names and performs a form check. If Tensorflow is in eager execution mode (interactive mode), the verification is carried out. Otherwise, nothing happens, which is practical when training models.\n",
        "\n",
        "The verification process consists in analyzing the form of the tensor and comparing each dimension with the given names. If an axis name is encountered for the first time, its length is added to the class cache. If an axis has already been seen, its current length is compared to the old one. If the lengths do not correspond, an error is lifted to report a dimension conflict.\n",
        "\n",
        "This class is a precious tool to make sure that the data complies with the specifications of the model, which makes it possible to avoid potential errors during training and prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmyf9d2o7xPY"
      },
      "outputs": [],
      "source": [
        "# This class makes it possible to check the forms of the tensors when handling data\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of each axis name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    # If Tensorflow is not in eager mode, does nothing\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    # Analyze the form of the tensor and compare it with the names given\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "\n",
        "      # If the new dimension is 1 and it must be disseminated, continues\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      # If the name of the axis is new, add its length to the cache\n",
        "      if old_dim is None:\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      # If the new dimension does not correspond to the old one, lift an error\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q4hdKkJ_KmM"
      },
      "source": [
        "## Download the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD3Sxk1axmel"
      },
      "source": [
        "In this sub-part, we download a file containing the data for our English-French translation model. The data comes from a text file in TSV format (TAB-SEPARATED VALUES), where each line represents a pair of sentences (English and French) separated by a tab.\n",
        "\n",
        "We use the `Pathlib` library to manage file paths and the function` tf.keras.utils.get_file () `to download the file. The download link points to a commonly used translation dataset.\n",
        "\n",
        "Once the file has been downloaded, we load the data by reading the text, dividing the lines and separating the English and French sentences to form lists of contexts (`Context_raw`) and targets (` Target_raw`). We then display the last French and English sentence to verify that the loading of the data has been carried out correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQKxkeZ_7xR3"
      },
      "outputs": [],
      "source": [
        "# Directly specific the path to the 'fra.txt' file\n",
        "import pathlib\n",
        "path_to_file = 'fra.txt'\n",
        "\n",
        "# Load the data from the specific file\n",
        "def load_data(path):\n",
        "    # Read the text file\n",
        "    text = pathlib.Path(path).read_text(encoding='utf-8')\n",
        "\n",
        "    # Split the text into lines\n",
        "    lines = text.splitlines()\n",
        "    # Split Each Line Into Peirs Sentence (English, French)\n",
        "    pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "    # Separate the English and French Sentences\n",
        "    target_raw = [pair[0] for pair in pairs]\n",
        "    context_raw = [pair[1] for pair in pairs]\n",
        "\n",
        "    return target_raw, context_raw\n",
        "\n",
        "# Load the data and display the last french and English sentence\n",
        "target_raw, context_raw = load_data(path_to_file)\n",
        "print(context_raw[-1])\n",
        "print(target_raw[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7I7-WAi_T7l"
      },
      "source": [
        "## Training and validation datases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1oqiW0Pxn0x"
      },
      "source": [
        "In this sub-part, we prepare the data for the training of our translation model. We divide the data into two sets: a training set and a validation set.\n",
        "\n",
        "We define two sets of Booléens, `is_train` and` is_val`, using a uniform random distribution to distribute the examples between the two sets. About 80 % of the examples are intended for training (`is_train = true`) and the rest is intended for validation (` is_val = false`).\n",
        "\n",
        "We then use these sets of Booléens to extract the indices from the training and validation examples from the lists `Context_raw` and` Target_raw`.\n",
        "\n",
        "Finally, we create two Tensorflow datasets (`Train_raw` and` Val_raw`) using the indices corresponding to the training and validation sets. These datasets will be used to train and validate our translation model.\n",
        "\n",
        "Note that we mix the examples when creating datasets to guarantee variability during training. We also bring together the examples in lots (`batch_size`) to improve the efficiency of the training process.\n",
        "\n",
        "In conclusion, this part prepares the data for our English-French translation model, dividing data into training and validation sets, and transforming it into Tensorflow datasets ready to be used in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhePE93N7xUQ"
      },
      "outputs": [],
      "source": [
        "# Settings for data preparation\n",
        "BUFFER_SIZE = len(context_raw)  # Buffer size for data mixing.\n",
        "                               # Here, it is defined as the total length of the data to ensure a complete mixture.\n",
        "BATCH_SIZE = 64                 # Number of examples to be treated at once during training.\n",
        "\n",
        "# Create a table of Booleans to determine whether an example should be used for training.\n",
        "# True value means that the example is intended for training, while a false value\n",
        "# means that it is intended for validation. 80 % of examples are intended for training.\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "# Extract indices from training and validation examples.\n",
        "train_indices = np.where(is_train)[0]\n",
        "val_indices = np.where(~is_train)[0]\n",
        "\n",
        "# Create datasets for training and validation using the previously determined indices.\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((np.array(context_raw)[train_indices], np.array(target_raw)[train_indices]))  # Create a dataset from paintings\n",
        "    .shuffle(BUFFER_SIZE)  # Mix examples to guarantee variability during training\n",
        "    .batch(BATCH_SIZE))    # Group the examples in lots (batches) for training\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((np.array(context_raw)[val_indices], np.array(target_raw)[val_indices]))  # Likewise for validation data\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))\n",
        "\n",
        "# Show some examples of the training set to check the format\n",
        "for example_context_strings, example_target_strings in train_raw.take(1):  # Take a lot of the training dataset\n",
        "  print(example_context_strings[:5])  # Show the first 5 contexts in the Lot\n",
        "  print()\n",
        "  print(example_target_strings[:5])  # Show the first 5 corresponding targets\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVEIuv7b_pJx"
      },
      "source": [
        "# Pre -treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKU39RIk_sMG"
      },
      "source": [
        "## Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ortHIPfqxpz7"
      },
      "source": [
        "In this sub-part, we carry out the pre-treatment of the text before using it to train our translation model. The purpose of pre -treatment is to normalize the text by eliminating the variations due to special and scrapyal characters, and transforming it into a tokens sequence.\n",
        "\n",
        "We use the example of text \"Are you an artificial intelligence researcher?\" To illustrate the standardization process. We first use the function `tf_Text.normalize_utf8 ()` to decompose the characters into their compatible forms (NFKD) and transform them into a normalized unicode text. Then, we convert the text into tiny and delete any character which is not a letter from the English alphabet, a space, a point, a question mark, a comma or an exclamation point. We also add spaces around punctuation to separate them like separate tokens.\n",
        "\n",
        "Finally, we add special tokens `[start]` and `[end]` around the text to indicate the beginning and the end of the tokens sequence. This step is essential for translation models so that they know when to start and finish the generation of text.\n",
        "\n",
        "The pre -treatment standardizes the text and transforms it into a tokens sequence ready to be used by the translation model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wNdHQh-7xWu"
      },
      "outputs": [],
      "source": [
        "# Example of text to transform\n",
        "example_text = tf.constant('Êtes-vous un chercheur en Intelligence Artificielle ?')\n",
        "\n",
        "# Show the initial text\n",
        "print(example_text.numpy())\n",
        "# Normalize the text to eliminate variations due to special or breakage characters, for example.\n",
        "# Here, 'NFKD' is a unicode type of standardization that breaks down the characters in their compatible forms.\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rax9w01m753f"
      },
      "outputs": [],
      "source": [
        "# Function to transform the text: put it into tiny and separate punctuation\n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Separate accentuated characters\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  # Convert the text into a lowercase\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep the space, the letters from A to Z, and certain punctuation signs\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Delete superfluous spaces\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  # Add tokens start and end around the text\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "# Show the transformed example\n",
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxP209AA_39t"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIWiYPuwxrOU"
      },
      "source": [
        "In this sub-part, we carry out the vectorization of the textual data to prepare the entry of our translation model. Vectorization consists in converting the words into digital indices (tokens) using a vocabulary dictionary.\n",
        "\n",
        "We define a maximum size for vocabulary (`Max_vocab_size`) which will limit the number of words taken into account for indexing. Then we create two text processors, one for the context (English) and the other for the target (French).\n",
        "\n",
        "Each processor is a layer of text vectorization, which takes the normalized text as input (from the previous sub-part) and transforms it into tokens sequences. We use the function `tf.keras.layers.textvectorization` for this, by specifying the normalization function, the maximum size of the vocabulary and the option` ragged = true` to indicate that the sequences will have variable lengths.\n",
        "\n",
        "Then, we \"adapt\" the text processors to the training data using the `.adapt () method` with the training datase. This allows text processors to learn vocabulary using training data.\n",
        "\n",
        "We then display the first 10 words of the vocabulary to verify what the text processors have learned.\n",
        "\n",
        "Finally, we use text processors to convert the examples of contextual chains into digital tokens sequences. This allows us to check if the tokenization is working properly and displaying a visual representation of the generated tokens and their mask (where the tokens are present and where they are not).\n",
        "\n",
        "Vectorization prepares the textual data by converting the words into digital indices (tokens) for treatment with the translation model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uoMuasC756F"
      },
      "outputs": [],
      "source": [
        "# We define a maximum size for vocabulary.\n",
        "max_vocab_size = 5000\n",
        "\n",
        "# We create a text processor for the context (English). This processor is a layer\n",
        "# text vectorization which allows the texts to be converted into tokens sequences.\n",
        "context_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct, # Function to normalize the text\n",
        "    max_tokens=max_vocab_size,            # Maximum vocabulary size\n",
        "    ragged=True)                          # Returns a variable tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTrOY3PJ758S"
      },
      "outputs": [],
      "source": [
        "# The text processor is \"suitable\" for training data. It's like adjusting a\n",
        "# Tokenizer on data: he learns vocabulary.\n",
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "# Displays the first 10 words of the vocabulary to check what he learned.\n",
        "context_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZsOp1u-75_a"
      },
      "outputs": [],
      "source": [
        "# In the same way, we create a text processor for the target (French).\n",
        "target_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)\n",
        "\n",
        "# Adapt the text processor to the targets of the training dataset.\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
        "target_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_wUv4x076Dg"
      },
      "outputs": [],
      "source": [
        "# Example of tokens generated by the text processor. We take some examples\n",
        "# of contextual channels and we tokenize them.\n",
        "example_tokens = context_text_processor(example_context_strings)\n",
        "example_tokens[:3, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcZMqCqB7xZL"
      },
      "outputs": [],
      "source": [
        "# Converts tokens into words using the vocabulary. This allows us to\n",
        "# Check if the tokenization is working properly.\n",
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duJQrvKN7xbq"
      },
      "outputs": [],
      "source": [
        "# Displays a visual representation of the IDS of Tokens and their mask.\n",
        "# The mask indicates where the tokens are present (value 1) and where they are not (value 0).\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens.to_tensor())\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens.to_tensor() != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3EmkDSX8Bwl"
      },
      "outputs": [],
      "source": [
        "# Function to process text before providing it to the model. She converts\n",
        "# the context and the target in tokens, and also creates inputs and exits\n",
        "# for the target (by shifting from a token).\n",
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "# Apply this treatment function to training and validation dataets.\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlKxMx0N8B_X"
      },
      "outputs": [],
      "source": [
        "# Displays an example of data processed to check the structure of tokens.\n",
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "  print(ex_context_tok[0, :10].numpy())\n",
        "  print()\n",
        "  print(ex_tar_in[0, :10].numpy())\n",
        "  print(ex_tar_out[0, :10].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGJQGeDg_8hT"
      },
      "source": [
        "# The encoder/decoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOh77_qI8CBw"
      },
      "outputs": [],
      "source": [
        "# The number of units for encoding and attention layers are defined.\n",
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LV4y_o5xtOW"
      },
      "source": [
        "In this part, we define the English-French translation model using the architecture of encoder-decoder. This model is an architecture commonly used for translation and text generation tasks.\n",
        "\n",
        "We use layers of encoding and attention to capture semantic information from input sentences and generate output translations.\n",
        "\n",
        "- `Units` represents the number of units (neurons) for layers of encoding and attention. This parameter is defined at 256, but it can be adjusted according to the needs and complexity of the model.\n",
        "\n",
        "The model of encoder-decooder is a powerful architecture which can be led to generate high-quality translations according to the pairs of training sentences. The combination of the vectorization of the text, the pre -treatment and the architecture of the model will make it possible to obtain an effective and precise translation system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk_b0yvfCe87"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCFfbXe3xvac"
      },
      "source": [
        "In this sub-part, we implement the personalized encoder layer. The encoder transforms a text sequence into a sequence of vectors. It uses two main components: the embedding layer and a bidirectional RNN layer.\n",
        "\n",
        "- The embedding layer (`self.embedding`) is used to convert tokens (words) into dense vectors. This makes it possible to represent the text continuously and facilitates learning the relationships between words. The embedding is specified by the number of units (`units`) that each word will be represented.\n",
        "\n",
        "- The bidirectional RNN layer (`Self.rnn ') treats sequentially embedding vectors. She takes the vectors from Embedding as an entry and returns a sequence of hidden states, making it possible to capture the contextual information in the two directions (front and rear) of the text. The `Merge_Mode option = 'Sum'' means that the outputs of the two directions are summoned.\n",
        "\n",
        "The method of the encoder takes an input sequence `X 'and performs the following operations:\n",
        "\n",
        "1. Conversion of tokens into embedding vectors using the embedding layer.\n",
        "2. Treatment of embedding vectors with the bidirectional RNN layer (`Self.rnn`).\n",
        "3. Reference of the processed sequence.\n",
        "\n",
        "The `Convert_input` method is used to convert a raw text to its encoded representation using the encoder. It takes a text as a text, converts it to tokens and passes it to the encoder to obtain the corresponding embedding vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrKYehzg8CES"
      },
      "outputs": [],
      "source": [
        "# The encoder is a personalized layer that converts a text sequence into a sequence of vectors.\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  # Initialization of the encoder\n",
        "  def __init__(self, text_processor, units):\n",
        "    # Initialization of the superclass\n",
        "    super(Encoder, self).__init__()\n",
        "    # Text processing is necessary to tokenize the text\n",
        "    self.text_processor = text_processor\n",
        "    # The size of the vocabulary determines the number of different words that can be treated\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    # The number of units in the RNN and Embedding layers\n",
        "    self.units = units\n",
        "\n",
        "    # The embedding layer converts tokens (words) into vectors\n",
        "    # This allows a dense representation of the text\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
        "\n",
        "    # The RNN layer (Gru here) deals with these sequential vectors\n",
        "    # Bidirectional means that the RNN treats the text in the two directions (front and rear)\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "        merge_mode='sum',\n",
        "        layer=tf.keras.layers.GRU(units, return_sequences=True, recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  # This method is called to treat an input sequence X\n",
        "  def call(self, x):\n",
        "    # An instance to check the shape of the tensors\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # Conversion of tokens into vectors\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # Treatment of vectors with RNN\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # Refer the treated sequence\n",
        "    return x\n",
        "\n",
        "  # This method converts a raw text into its encoded representation\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bvCWFAq8CGT"
      },
      "outputs": [],
      "source": [
        "# Encode the entry sequence\n",
        "\n",
        "# Instancing of the encoder\n",
        "encoder = Encoder(context_text_processor, UNITS)\n",
        "ex_context = encoder(ex_context_tok)\n",
        "\n",
        "print(f'Tokens de contexte, forme (batch, s): {ex_context_tok.shape}')\n",
        "print(f'Sortie de l\\'encodeur, forme (batch, s, units): {ex_context.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITocyJnvDSiD"
      },
      "source": [
        "## The attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jRbWfuexxN6"
      },
      "source": [
        "In this sub-part, we define the personalized attention layer (`Crossatant '). This layer allows the model to be focused on certain parts of the context during translation.\n",
        "\n",
        "- The layer uses `tf.keras.layers.multihedatet '(` self.mha`) which is a method of attention to process information in several ways at the same time.\n",
        "- `Self.Layernorm` is a layer of normalization which improves the stability of learning.\n",
        "- `Self.add` combines the outputs of attention with the previous outings.\n",
        "\n",
        "The method of the attention layer takes a sequence `X 'as input and the context` context. She performs the following operations:\n",
        "\n",
        "1. Obtaining attention weights and output using `TF.Keras.Layers.Multiheadatet '.\n",
        "2. Combination of outputs using `Self.add`.\n",
        "3. Normalization of outputs using `Self.Layernorm`.\n",
        "\n",
        "This layer is used in the decoder to focus on the relevant context during the generation of translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhkyLk8R8CI0"
      },
      "outputs": [],
      "source": [
        "# This layer provides a attention mechanism to focus on certain parts of the context during the translation\n",
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    # Multiheadatetation is a method of attention that processes information in several ways both\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        "\n",
        "    # Obtaining attention weights and exit\n",
        "    attn_output, attn_scores = self.mha(query=x, value=context, return_attention_scores=True)\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    # Combination of outings\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "# Assist the encoded tokens to check the operation of the attention layer\n",
        "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(), output_dim=UNITS, mask_zero=True)\n",
        "ex_tar_embed = embed(ex_tar_in)\n",
        "result = attention_layer(ex_tar_embed, ex_context)\n",
        "\n",
        "# Display of forms to understand the transformation of data\n",
        "print(f'Séquence de contexte, forme (batch, s, units): {ex_context.shape}')\n",
        "print(f'Séquence cible, forme (batch, t, units): {ex_tar_embed.shape}')\n",
        "print(f'Résultat de l\\'attention, forme (batch, t, units): {result.shape}')\n",
        "print(f'Poids de l\\'attention, forme (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QyV_kQ98CLT"
      },
      "outputs": [],
      "source": [
        "# Calculate the sum of the weight of attention to verify that they summon well to 1\n",
        "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnSK5aCu8NLK"
      },
      "outputs": [],
      "source": [
        "# Visualization of attention weights\n",
        "\n",
        "# Recovery of attention weights from the last attention layer\n",
        "attention_weights = attention_layer.last_attention_weights\n",
        "# Creation of a mask to exclude tokens which are zeros (that is to say padding)\n",
        "mask=(ex_context_tok != 0).numpy()\n",
        "\n",
        "# Creation of a figure with 2 subpups side by side\n",
        "plt.subplot(1, 2, 1)\n",
        "# Display of attention weights multiplied by the mask (to delete the display of paddings)\n",
        "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
        "plt.title('Poids de l\\'attention')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Mask display itself to view the actually hidden regions\n",
        "plt.pcolormesh(mask)\n",
        "plt.title('Masque');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e0wcqtwDjJu"
      },
      "source": [
        "## The decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDFzXUTsxzFo"
      },
      "source": [
        "In this sub-part, we implement the class of the personalized decoder. The decoder takes the encoded context and generates a target sequence (translation) word by word.\n",
        "\n",
        "The decoder has several important components:\n",
        "\n",
        "- `Self.word_to_id` and` Self.id_To_Word` are layers of conversion of words into unique identifiers and vice versa. They are used to manage the vocabulary of the target sequences.\n",
        "\n",
        "- `Self.start_token 'and` Self.end_token' represent the identifiers of the tokens of the start and end of the sequence. They are used to indicate when to start and stop the translation generation.\n",
        "\n",
        "- `Self.embedding` is an embedding layer to convert tokens identifiers into embedding vectors.\n",
        "\n",
        "- `Self.rnn` is an RNN layer (GRU) used to treat target sequences.\n",
        "\n",
        "- `Self. Attention 'is the attention layer (` Crossatant') used to focus on the context during the generation of the translation.\n",
        "\n",
        "- `SELF.Output_layer` is a dense layer which predicts the next token according to the outputs of the decoder.\n",
        "\n",
        "The method `Call` Du Décumer takes the context encoded in the context of context, the input tokens` X`, the state of the decoder `State 'and an option` Return_state`. She performs the following operations:\n",
        "\n",
        "1. Conversion of input tokens into embedding vectors using the embedding layer.\n",
        "2. Treatment of the input sequence with the RNN layer to obtain the X 'output sequence.\n",
        "3. Using the attention layer to focus on the relevant context during the generation of translation.\n",
        "4. Prediction of the next token with the output layer.\n",
        "\n",
        "The `Get_initial_State` method is used to initialize the condition of the decoder before translation. It returns the token at the start for each batch sequence, initializes the \"Done\" variable to false for all the sequences and returns the initial state of the RNN.\n",
        "\n",
        "The `Tokens_To_Text 'method converts the tokens to text, using the reverse dictionary` Self.id_to_word`. She joins the words to form a sentence and removes the tokens at the start and end.\n",
        "\n",
        "The `Get_Next_token` method is used to predict the next token during the translation generation. It takes the context asset, the next token, the state of the decoder, the \"Done\" variable (indicating if a sequence is finished) and a temperature option for the random generation. If the temperature is equal to 0, the token is chosen with the highest probability (deterministic mode). Otherwise, the token is chosen randomly depending on the logits (stochastic mode). This method is used to iterate on tokens and generate the complete word translation by word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC0Bge_z8NNm"
      },
      "outputs": [],
      "source": [
        "# Definition of the decoder class\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  # Define a class method to dynamically add methods to the class\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  # Initialization of the decoder class\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()  # Initializing the Layer Superclass\n",
        "    # Text Processor to treat target sequences\n",
        "    self.text_processor = text_processor\n",
        "    # Vocabulary size of target sequences\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    # Word conversion into unique identifiers\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "    # Reverse conversion of unique identifiers in words\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "    # Identify the start and end token\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "    # Define the number of units for the RNN and the embedding layer\n",
        "    self.units = units\n",
        "    # Convert tokens identifiers into embedding vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero=True)\n",
        "    # RNN to treat target sequences\n",
        "    self.rnn = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    # Attention layer to focus on the relevant context\n",
        "    self.attention = CrossAttention(units)\n",
        "    # Outing layer to predict the next token\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CstxLWuJ8NPz"
      },
      "outputs": [],
      "source": [
        "# The \"call\" method to treat sequences with the decoder\n",
        "@Decoder.add_method\n",
        "def call(self, context, x, state=None, return_state=False):\n",
        "  shape_checker = ShapeChecker()  # Utility to check the dimensions of the tensors\n",
        "  # Check the dimensions of the input Tensor\n",
        "  shape_checker(x, 'batch t')\n",
        "  shape_checker(context, 'batch s units')\n",
        "  # Convert tokens identifiers into embedding vectors\n",
        "  x = self.embedding(x)\n",
        "  shape_checker(x, 'batch t units')\n",
        "  # Treat the sequence with the RNN\n",
        "  x, state = self.rnn(x, initial_state=state)\n",
        "  shape_checker(x, 'batch t units')\n",
        "  # Use the attention layer to focus on the relevant context\n",
        "  x = self.attention(x, context)\n",
        "  # Save attention weights for possible visualization\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "  shape_checker(x, 'batch t units')\n",
        "  shape_checker(self.last_attention_weights, 'batch t s')\n",
        "  # Predict the next token with the output layer\n",
        "  logits = self.output_layer(x)\n",
        "  shape_checker(logits, 'batch t target_vocab_size')\n",
        "  # Return either the logits with the state or just the logits according to \"Return_state\"\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYZTLR_f8NSO"
      },
      "outputs": [],
      "source": [
        "# Instantly decoder with the appropriate parameters\n",
        "decoder = Decoder(target_text_processor, UNITS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIAYjB-i8CNR"
      },
      "outputs": [],
      "source": [
        "# Test the decoder with an example of a context and an entry sequence\n",
        "logits = decoder(ex_context, ex_tar_in)\n",
        "\n",
        "# Show tense forms to make sure they are correct\n",
        "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
        "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UzPfCaL8So5"
      },
      "outputs": [],
      "source": [
        "# Definition of additional methods for the decoder\n",
        "\n",
        "# This method initializes the state of the decoder before the translation\n",
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  # Get batch size from the context\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  # Create the starting token for each batch sequence\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  # Initializing the \"Done\" variable to false for all sequences\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  # Convert the starting tokens to embeddings\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  # Return the starting token, the \"Done\" variable, and the initial state of the RNN\n",
        "  # Fix: call get_initial_state with the batch size\n",
        "  return start_tokens, done, self.rnn.get_initial_state(batch_size=batch_size)[0]\n",
        "\n",
        "# Convert tokens into text\n",
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  # Convert tokens identifiers into words\n",
        "  words = self.id_to_word(tokens)\n",
        "  # Join the words to form a sentence\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  # Remove the departure and end tokens\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result\n",
        "\n",
        "# Predict the next token\n",
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  # Obtain the logits and the condition of the decoder\n",
        "  logits, state = self(\n",
        "    context, next_token,\n",
        "    state = state,\n",
        "    return_state=True)\n",
        "\n",
        "  # If the temperature is equal to 0, choose the token with the highest probability\n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    # Otherwise, choose a token randomly depending on the logits\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If an end token is generated, update the \"Done\" variable\n",
        "  done = done | (next_token == self.end_token)\n",
        "  # If a sequence is finished, produce only padding\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "  return next_token, done, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaLIgphA8SrR"
      },
      "outputs": [],
      "source": [
        "# Initialize the variables for the translation loop\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "# Loop to generate a translation\n",
        "for n in range(10):\n",
        "  # Get the next token\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Add the token to the tokens list\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Concatenate all tokens to obtain the complete translation\n",
        "tokens = tf.concat(tokens, axis=-1) # (Batch, t)\n",
        "\n",
        "# Convert tokens into text\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "result[:3].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4_jXHj3Dqzi"
      },
      "source": [
        "# The model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK0RD0I-x0ty"
      },
      "source": [
        "In this part, we create a class `Translator` to combine the encoder and the decoder in a complete translation model. The class `Translator` inherits from` tf.keras.model ', which allows us to define the method `Calle' to execute the translation.\n",
        "\n",
        "The `Calle method takes a tuple` inputs \"containing the context and the target sequence` X '. She performs the following operations:\n",
        "\n",
        "1. Execute the encoder to obtain the context encoded from the input context (`Context = Self.coder (Context)`).\n",
        "2. Execute the decoder to obtain the output logits using the encoded context and the target sequence (`Logits = self.Decoder (Context, x)`).\n",
        "\n",
        "We also define a translation method (`translate`) for the translator 'model. This method takes a raw text as input and returns its translation using the trained model. It uses the encoder to convert the raw text to its encoded representation (`context = self.coder.convert_input (texts)`), then uses the decoder to generate the word translation by using the method `get_next_token 'of the decoder.\n",
        "\n",
        "We also add a method to visualize attention during the translation (`Plot_Atation`). This method takes a raw text as input, uses the model to translate it, and displays attention weights on a matrix to visualize which parts of the context were used to generate each word of the translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ndxCJis8Stq"
      },
      "outputs": [],
      "source": [
        "# Translator class to combine the encoder and the decoder\n",
        "class Translator(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and the decoder\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  # Definition of the \"Call\" method to execute the translation\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    # Execute the encoder to obtain the encoded context\n",
        "    context = self.encoder(context)\n",
        "    # Execute the decoder to obtain the logits\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    # Deletion of the Keras mask (specific note note, probably linked to a technical constraint)\n",
        "    try:\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tzdO7A-8Sv_"
      },
      "outputs": [],
      "source": [
        "# Creation of an instance of the Translator model and test on an example\n",
        "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
        "\n",
        "logits = model((ex_context_tok, ex_tar_in))\n",
        "\n",
        "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
        "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwBXWNBpDthC"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxSon2-bx2Aq"
      },
      "source": [
        "In this sub-part, we define the personalized loss and precision functions for the model, adapted to the training process of the sequence sequence model.\n",
        "\n",
        "- The `Masked_loss' function calculates the loss by ignoring padding tokens. This is necessary because the target sequences can have different lengths and are supplemented by padding tokens. By masking these padding tokens, the loss is only calculated on the relevant tokens.\n",
        "\n",
        "- The `Masked_acc` function calculates the precision by also ignoring padding tokens. Like masked loss, masked accuracy ensures that metric is calculated only on relevant tokens and ignores padding tokens.\n",
        "\n",
        "Then we comply the model using an optimizer, the masked loss function and the masked metrics. We use Adam optimizer for weight updates, and we use masked loss and precision functions to assess the model errors and monitor your performance during training.\n",
        "\n",
        "Finally, we assess the performance of the model on a validation dataset to see its initial performance before training.\n",
        "\n",
        "We then use the `FIT method to train the model on training data. We repeat the training data for several eras and use validation to monitor performance. We also use the TF.Keras.keras.Callbacks.earlyStopping` Reminder to stop training if the loss does not improve for a number of consecutive times. This avoids unnecessary switching on and allows us to choose the best model based on performance on validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgWxESNN8SyZ"
      },
      "outputs": [],
      "source": [
        "# Define the loss and precision functions for the model\n",
        "\n",
        "\n",
        "# The \"masked_loss\" function is designed to calculate the loss by ignoring padding tokens.\n",
        "# This is necessary because when training a sequence sequence model,\n",
        "# The sequences can have different lengths and are supplemented by padding tokens.\n",
        "# By masking these padding tokens, we make sure that the loss is only calculated on the relevant tokens.\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculation of the loss for each batch element\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the loss on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # We return the result\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "# The \"masked_acc\" function is designed to calculate precision by also ignoring padding tokens.\n",
        "# Like masked loss, masked accuracy ensures that metric is calculated only on\n",
        "# The relevant tokens and ignores padding tokens.\n",
        "\n",
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculation of the loss for each batch element\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00J0x_xk8S0x"
      },
      "outputs": [],
      "source": [
        "# Compile the model with an optimizer, a loss function and metrics\n",
        "\n",
        "# The compile method configures the model learning process\n",
        "model.compile(optimizer='adam', # Uses the \"Adam\" optimizer for weight updates\n",
        "              loss=masked_loss, # Personalized loss function to assess the errors of the model\n",
        "              metrics=[masked_acc, masked_loss]) # Metrics to monitor performance during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzNoZJas8S1z"
      },
      "outputs": [],
      "source": [
        "# Calculate the target vocabulary size\n",
        "\n",
        "# Obtain vocabulary size from the text processor\n",
        "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
        "\n",
        "# Calculate the expected loss and precision for a model that predicts random outings\n",
        "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        " \"expected_acc\": 1/vocab_size}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOfrjzER8cMm"
      },
      "outputs": [],
      "source": [
        "# Evaluate the performance of the model on a validation dataset\n",
        "\n",
        "# Assesses the current performance of the validation data model\n",
        "model.evaluate(val_ds, steps=20, return_dict=True)\n",
        "\n",
        "# Make the model\n",
        "\n",
        "# Use the \"Fit\" method to train the model on training data\n",
        "history = model.fit(\n",
        "    train_ds.repeat(), # Repeat training data for several passages (eras)\n",
        "    epochs=15, # Number of training times\n",
        "    steps_per_epoch = 20, # Number of lots treated before going to the next time\n",
        "    validation_data=val_ds, # Data used for validation\n",
        "    validation_steps = 20, # Number of validation lots to be used in each era\n",
        "    callbacks=[ # Mechanisms to intervene during training\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3)]) # Stop training if the loss does not improve for 3 consecutive times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd5blPXMCRjd"
      },
      "source": [
        "## Visualization of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV0gACWOx3vk"
      },
      "source": [
        "In this sub-part, we visualize the results of the training by displaying the loss and precision curves during training. This allows us to see how loss and precision evolve as and when the times, which is useful for assessing the performance of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpUr8Fhu-KnH"
      },
      "outputs": [],
      "source": [
        "# Show the loss curve during training\n",
        "plt.plot(history.history['loss'], label='loss') # Loss of training curve\n",
        "plt.plot(history.history['val_loss'], label='val_loss') # Validation loss curve\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch # ') # Axis of abscissa\n",
        "plt.ylabel('CE/token') # Order axis\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nnzh1G7DyJF"
      },
      "outputs": [],
      "source": [
        "# Show the precision curve during training\n",
        "plt.plot(history.history['masked_acc'], label='accuracy') # Training precision curve\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy') # Validation precision curve\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch # )\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aTjvpR9D2ec"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAnCEum0x5GA"
      },
      "source": [
        "Finally, in this sub-part, we add a translation method to the class `Translator`. This method takes on a raw text as input and returns its translation using the trained model.\n",
        "\n",
        "We use the encoder to convert raw text to its encoded representation. Then, we initialize the variables for the generation loop of tokens and iterons to generate the next token until reaching the maximum length of the output or until all the sequences are finished. We use the `Get_Next_Token 'method of the decoder to generate the next token according to the attention weights calculated by the attention layer. Finally, we concrete the tokens lists to obtain the complete translation and send it back.\n",
        "\n",
        "We also add a method `Plot_Antet 'to view the weights of attention during the translation. This method takes a raw text as input, uses the model to translate it, and displays attention weights on a matrix to visualize which parts of the context were used to generate each word of the translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zowbzAUA8cO8"
      },
      "outputs": [],
      "source": [
        "# Add a translation method to the Translator class\n",
        "\n",
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts, *,\n",
        "              max_length=50, # Maximum output length\n",
        "              temperature=0.0): # Parameter to control the diversity of the output\n",
        "  # Converts the raw text into encoded representation\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  batch_size = tf.shape(texts)[0] # Obtain the prize size\n",
        "\n",
        "  # Initializing the variables for the tokens generation loop\n",
        "  tokens = []\n",
        "  attention_weights = []\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    # Generate the following token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done, state, temperature)\n",
        "\n",
        "    # Add the token and attention weights to their respective lists\n",
        "    tokens.append(next_token)\n",
        "    attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "    # Stop the generation if all the texts are finished\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Concaten the lists of tokens and weight of attention\n",
        "  tokens = tf.concat(tokens, axis=-1)\n",
        "  self.last_attention_weights = tf.concat(attention_weights, axis=1)\n",
        "\n",
        "  # Convert tokens into text\n",
        "  result = self.decoder.tokens_to_text(tokens)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsqFEyhr8cRO"
      },
      "outputs": [],
      "source": [
        "# Test the translation method\n",
        "\n",
        "# Translate an example of a sentence\n",
        "result = model.translate(['J\\'aime les pommes.'])\n",
        "# Show the translation\n",
        "result[0].numpy().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ46Tc768cTc"
      },
      "outputs": [],
      "source": [
        "# Add a method to view attention when translation\n",
        "\n",
        "@Translator.add_method\n",
        "def plot_attention(self, text, **kwargs):\n",
        "  # Make sure the text is a chain\n",
        "  assert isinstance(text, str)\n",
        "  # Obtain the translation of the text\n",
        "  output = self.translate([text], **kwargs)\n",
        "  output = output[0].numpy().decode()\n",
        "\n",
        "  attention = self.last_attention_weights[0] # Recover attention\n",
        "\n",
        "  # PRETRATION OF TEXTS for display\n",
        "  context = tf_lower_and_split_punct(text)\n",
        "  context = context.numpy().decode().split()\n",
        "  output = tf_lower_and_split_punct(output)\n",
        "  output = output.numpy().decode().split()[1:]\n",
        "\n",
        "  # Create a graphic to view attention weights\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "  fontdict = {'fontsize': 14}\n",
        "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.set_xlabel('Texte en entrée')\n",
        "  ax.set_ylabel('Texte en sortie')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhtKoelxEgUH"
      },
      "source": [
        "## Visualize attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHsLlpn6x6ZB"
      },
      "source": [
        "In this sub-part, we test the visualization of attention weights during the translation. We use the method `Plot_Antet 'of the model to display attention weights between the input text and the generated translation. We first test on a short example, then on a longer text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMU21cKlEer_"
      },
      "outputs": [],
      "source": [
        "# Test the visualization of attention\n",
        "model.plot_attention('J\\'aime les pommes rouges.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYluGTZm8cVu"
      },
      "outputs": [],
      "source": [
        "# Recover a long text for the test\n",
        "long_text = context_raw[-1]\n",
        "\n",
        "# Show the expected translation\n",
        "import textwrap\n",
        "print('Expected outing: \\ n', '\\n'.join(textwrap.wrap(target_raw[-1])))\n",
        "\n",
        "# Test the visualization of attention on the long text\n",
        "model.plot_attention(long_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK6psm__8CPs"
      },
      "outputs": [],
      "source": [
        "# Prepare some entries for additional tests\n",
        "inputs = [\n",
        "    'J\\'aime les pommes rouges.',\n",
        "    'Qui es tu ?',\n",
        "    'Tu es très jolie aujourd\\'hui.',\n",
        "    'Quand viendras tu ?'\n",
        "]\n",
        "\n",
        "for t in inputs:\n",
        "  print(model.translate([t])[0].numpy().decode())\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1M27RWVQt7"
      },
      "source": [
        "# Results analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghhgEhGXx74O"
      },
      "source": [
        "In this part, we examine the results of the training and perform different analyzes on the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SZ4B6T-WF3T"
      },
      "source": [
        "## Histogram of gradients\n",
        "\n",
        "This stud will give you an idea of ​​the magnitude of the gradients during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx9-m1XZx87r"
      },
      "source": [
        "We display a histogram of gradient standards to get an idea of ​​their magnitude during training. This can help us check whether the gradients are too small or too large, which could affect the convergence of training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B_GBjmaWDZC"
      },
      "outputs": [],
      "source": [
        "def plot_gradient_histogram(model):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model((ex_context_tok, ex_tar_in))\n",
        "        loss_value = masked_loss(ex_tar_out, logits)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    gradients = [tf.norm(grad).numpy() for grad in grads if grad is not None]\n",
        "\n",
        "    plt.hist(gradients, bins=50)\n",
        "    plt.xlabel('Gradient Norm')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of Gradients')\n",
        "    plt.show()\n",
        "\n",
        "# Example of use after each era or iteration:\n",
        "plot_gradient_histogram(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXXH_VUMaVAX"
      },
      "source": [
        "## Loss curve by Epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWCkI73mx_zo"
      },
      "source": [
        "We trace a curve to visualize the evolution of the loss of training and the loss of validation over the eras. This allows us to see how loss changes during training and identify any over-learning or under-learning trend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_99L-LWaNy6"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "    plt.plot(epochs, loss, 'bo', label='Perte (entraînement)')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Perte (validation)')\n",
        "    plt.title('Perte (entraînement et validation)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Perte')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1I7-PzZaumx"
      },
      "source": [
        "## Loss curve depending on time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bveYpjd7yCex"
      },
      "source": [
        "We record the time required for each training period and train a curve to visualize the loss according to the elapsed time. This allows us to see if the loss gradually decreases over time and if training is progressing effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4R72-NPawjH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def plot_loss_vs_time(history):\n",
        "    loss = history.history['loss']\n",
        "    timestamps = [time.time() - start_time for start_time in history.epoch]\n",
        "    plt.plot(timestamps, loss, 'bo-')\n",
        "    plt.title('Perte en fonction du temps')\n",
        "    plt.xlabel('Temps (secondes)')\n",
        "    plt.ylabel('Perte')\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_vs_time(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGtbbbHla1tP"
      },
      "source": [
        "## Histogram of losses distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocTKnXXSyEuQ"
      },
      "source": [
        "We draw a histogram to view the distribution of loss values ​​during training. This can give us an idea of ​​the variability of the loss and stability of training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOP0wJ7Ha3--"
      },
      "outputs": [],
      "source": [
        "def plot_loss_distribution(history):\n",
        "    loss = history.history['loss']\n",
        "    plt.hist(loss, bins=10)\n",
        "    plt.title('Distribution de la perte')\n",
        "    plt.xlabel('Perte')\n",
        "    plt.ylabel('Fréquence')\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_distribution(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIdn3Kb_yPgn"
      },
      "source": [
        "## Recording of results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMoSC0JLyQqP"
      },
      "source": [
        "Finally, we record the trained model using the Keras Save` method, as well as the training history in the form of a pickle file to be able to reuse them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGreU1lIyMmi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model with the .Keras extension\n",
        "model.save('nom_du_modele.keras')\n",
        "\n",
        "# Save history\n",
        "with open('historique.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}